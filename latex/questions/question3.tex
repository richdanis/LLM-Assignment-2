\begin{question}{
Prompting Language Models [Shridhar] \textbf{(15 pts + 2.5 (Bonus))}}

\paragraph{Overview:}
Modern prompting techniques with LLMs involve strategically designing input prompts to guide the model's behavior and improve its performance on specific tasks. These techniques have become increasingly important as LLMs  demonstrate remarkable capabilities when given appropriate prompts. %
In this question, we will try to explore the effect of these prompting variants for various language models (particularly, T5 and Flan-T5). 

In particular, in this question, we will explore the domain of math word problems which require a combination of text understanding and arithmetic reasoning.
We will test cases when training data vs. no training data is available for solving the task of solving math word problems.  In a practical setting, we will understand the usefulness of concepts like in-context learning, explicit instructions, instruction-tuned models, and step-by-step guidance. 


\paragraph{Instructions:}
The set of questions and our implementation pipeline is provided in the \href{https://colab.research.google.com/drive/1oj3uqSrS76vmVd1shKriCXaIYZJ9sGrB?usp=sharing}{Google Colab Notebook}. 
All task-related dependencies and step-by-step instructions are also provided in the Colab notebook. It is strongly recommended to use Google Colab for running all the experiments (except question 1 for which Euler GPUs will be needed) and download the Colab file in a \texttt{.ipynb} format and submit the file in the following naming: \texttt{Q3.ipynb}


\end{question}